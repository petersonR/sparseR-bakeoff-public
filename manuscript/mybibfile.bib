
@article{breiman,
	author = {Leo Breiman},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
volume = {16},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {199 -- 231},
year = {2001},
doi = {10.1214/ss/1009213726},
URL = {https://doi.org/10.1214/ss/1009213726}
}


@article{tibs1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society: Series B},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}


@article{srlpaper,
    title = {Ranked Sparsity: A Cogent Regularization Framework for Selecting and Estimating Feature Interactions and Polynomials},
    author = {Ryan A. Peterson and Joseph E. Cavanaugh},
    year = {2022},
    journal = {AStA Advances in Statistical Analysis},
    doi = {10.1007/s10182-021-00431-7},
    volume={106},
    pages={427--454},
  }
  
@article{romano2021pmlb,
  title={PMLB v1.0: an open source dataset collection for benchmarking machine learning methods},
  author={Romano, Joseph D and Le, Trang T and La Cava, William and Gregg, John T and Goldberg, Daniel J and Chakraborty, Praneel and Ray, Natasha L and Himmelstein, Daniel and Fu, Weixuan and Moore, Jason H},
  journal={arXiv preprint arXiv:2012.00058v2},
  year={2021}
}

@article{Olson2017PMLB,
    author="Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.",
    title="PMLB: a large benchmark suite for machine learning evaluation and comparison",
    journal="BioData Mining",
    year="2017",
    month="Dec",
    day="11",
    volume="10",
    number="36",
    pages="1--13",
    issn="1756-0381",
    doi="10.1186/s13040-017-0154-4",
    url="https://doi.org/10.1186/s13040-017-0154-4"
}

@article{mlvslr,
title = {A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models},
journal = {Journal of Clinical Epidemiology},
volume = {110},
pages = {12-22},
year = {2019},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2019.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0895435618310813},
author = {Evangelia Christodoulou and Jie Ma and Gary S. Collins and Ewout W. Steyerberg and Jan Y. Verbakel and Ben {Van Calster}},
keywords = {Clinical prediction models, Logistic regression, Machine learning, AUC, Calibration, Reporting},
abstract = {Objectives
The objective of this study was to compare performance of logistic regression (LR) with machine learning (ML) for clinical prediction modeling in the literature.
Study Design and Setting
We conducted a Medline literature search (1/2016 to 8/2017) and extracted comparisons between LR and ML models for binary outcomes.
Results
We included 71 of 927 studies. The median sample size was 1,250 (range 72–3,994,872), with 19 predictors considered (range 5–563) and eight events per predictor (range 0.3–6,697). The most common ML methods were classification trees, random forests, artificial neural networks, and support vector machines. In 48 (68%) studies, we observed potential bias in the validation procedures. Sixty-four (90%) studies used the area under the receiver operating characteristic curve (AUC) to assess discrimination. Calibration was not addressed in 56 (79%) studies. We identified 282 comparisons between an LR and ML model (AUC range, 0.52–0.99). For 145 comparisons at low risk of bias, the difference in logit(AUC) between LR and ML was 0.00 (95% confidence interval, −0.18 to 0.18). For 137 comparisons at high risk of bias, logit(AUC) was 0.34 (0.20–0.47) higher for ML.
Conclusion
We found no evidence of superior performance of ML over LR. Improvements in methodology and reporting are needed for studies that compare modeling algorithms.}
}

@article{datahungry, 
author = "van der Ploeg, T. and Austin, P.C. and Steyerberg, E.W.", 
title = {Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints},
journal = {BMC Med Res Methodol}, 
volume = {14}, 
pages = {137}, 
year = {2014}, 
doi = {https://doi.org/10.1186/1471-2288-14-137}
}
@article{zou2006adaptive,
  title={The adaptive lasso and its oracle properties},
  author={Zou, Hui},
  journal={Journal of the American statistical association},
  volume={101},
  number={476},
  pages={1418--1429},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@Article{rfsInR,
    title = {Classification and Regression by randomForest},
    author = {Andy Liaw and Matthew Wiener},
    journal = {R News},
    year = {2002},
    volume = {2},
    number = {3},
    pages = {18-22},
    url = {https://CRAN.R-project.org/doc/Rnews/},
  }
  
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@Manual{pmlbr,
    title = {pmlbr: Interface to the Penn Machine Learning Benchmarks Data
Repository},
    author = {Trang Le and {makeyourownmaker} and Jason Moore},
    year = {2023},
    note = {R package version 0.2.1},
    url = {https://CRAN.R-project.org/package=pmlbr},
  }
  
@Article{orq_paper,
    title = {Ordered quantile normalization: a semiparametric transformation built for the cross-validation era},
    author = {Ryan A. Peterson and Joseph E. Cavanaugh},
    year = {2020},
    journal = {Journal of Applied Statistics},
    publisher = {Taylor & Francis},
    pages = {2312-2327},
    doi = {10.1080/02664763.2019.1630372},
    volume = {47},
    number = {13-15},
  }
  
@Article{bestNormalize,
    title = {{Finding Optimal Normalizing Transformations via
          bestNormalize}},
    author = {Ryan A. Peterson},
    year = {2021},
    journal = {{The R Journal}},
    doi = {10.32614/RJ-2021-041},
    pages = {310--329},
    volume = {13},
    number = {1},
  }
  
@Article{caret,
    title = {Building Predictive Models in R Using the caret Package},
    volume = {28},
    url = {https://www.jstatsoft.org/index.php/jss/article/view/v028i05},
    doi = {10.18637/jss.v028.i05},
    number = {5},
    journal = {Journal of Statistical Software},
    author = {{Kuhn} and {Max}},
    year = {2008},
    pages = {1–26},
  }
  
@article{breheny,
    author = {Breheny, Patrick J},
    title = "{Marginal false discovery rates for penalized regression models}",
    journal = {Biostatistics},
    volume = {20},
    number = {2},
    pages = {299-314},
    year = {2018},
    month = {02},
    abstract = "{Penalized regression methods are an attractive tool for high-dimensional data analysis, but their widespread adoption has been hampered by the difficulty of applying inferential tools. In particular, the question “How reliable is the selection of those features?” has proved difficult to address. In part, this difficulty arises from defining false discoveries in the classical, fully conditional sense, which is possible in low dimensions but does not scale well to high-dimensional settings. Here, we consider the analysis of marginal false discovery rates (mFDRs) for penalized regression methods. Restricting attention to the mFDR permits straightforward estimation of the number of selections that would likely have occurred by chance alone, and therefore provides a useful summary of selection reliability. Theoretical analysis and simulation studies demonstrate that this approach is quite accurate when the correlation among predictors is mild, and only slightly conservative when the correlation is stronger. Finally, the practical utility of the proposed method and its considerable advantages over other approaches are illustrated using gene expression data from The Cancer Genome Atlas and genome-wide association study data from the Myocardial Applied Genomics Network.}",
    issn = {1465-4644},
    doi = {10.1093/biostatistics/kxy004}
}

@article{stacking,
title = {Stacked generalization},
journal = {Neural Networks},
volume = {5},
number = {2},
pages = {241-259},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800231},
author = {David H. Wolpert},
keywords = {Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction},
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
}

@book{ensembles,
  title={Ensemble methods: foundations and algorithms},
  author={Zhou, Zhi-Hua},
  year={2012},
  publisher={CRC press}
}

@phdthesis{peterson2019ranked,
  title={Ranked sparsity: a regularization framework for selecting features in the presence of prior informational asymmetry},
  author={Peterson, Ryan Andrew},
  year={2019},
  school={The University of Iowa}
}

@Manual{recipes,
    title = {recipes: Preprocessing and Feature Engineering Steps for Modeling},
    author = {Max Kuhn and Hadley Wickham and Emil Hvitfeldt},
    year = {2024},
    note = {R package version 1.0.10},
    url = {https://CRAN.R-project.org/package=recipes},
  }
  
@article{algbias,
  title={Algorithmic bias: review, synthesis, and future research directions},
  author={Kordzadeh, Nima and Ghasemaghaei, Maryam},
  journal={European Journal of Information Systems},
  volume={31},
  number={3},
  pages={388--409},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{alikhademi2022review,
  title={A review of predictive policing from the perspective of fairness},
  author={Alikhademi, Kiana and Drobina, Emma and Prioleau, Diandra and Richardson, Brianna and Purves, Duncan and Gilbert, Juan E},
  journal={Artificial Intelligence and Law},
  pages={1--17},
  year={2022},
  publisher={Springer}
}

@article{brotcke2022time,
  title={Time to assess bias in machine learning models for credit decisions},
  author={Brotcke, Liming},
  journal={Journal of Risk and Financial Management},
  volume={15},
  number={4},
  pages={165},
  year={2022},
  publisher={MDPI}
}

@article{yarger2020algorithmic,
  title={Algorithmic equity in the hiring of underrepresented IT job candidates},
  author={Yarger, Lynette and Cobb Payton, Fay and Neupane, Bikalpa},
  journal={Online information review},
  volume={44},
  number={2},
  pages={383--395},
  year={2020},
  publisher={Emerald Publishing Limited}
}

@article{obermeyer2019dissecting,
  title={Dissecting racial bias in an algorithm used to manage the health of populations},
  author={Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
  journal={Science},
  volume={366},
  number={6464},
  pages={447--453},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{friedman,
  title={Multivariate adaptive regression splines},
  author={Friedman, Jerome H},
  journal={The annals of statistics},
  volume={19},
  number={1},
  pages={1--67},
  year={1991},
  publisher={Institute of Mathematical Statistics}
}

@Article{lmertest,
    title = {{lmerTest} Package: Tests in Linear Mixed Effects Models},
    author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {82},
    number = {13},
    pages = {1--26},
    doi = {10.18637/jss.v082.i13},
}

@article{apnea,
  title={Diagnosis of sleep apnea by automatic analysis of nasal pressure and forced oscillation impedance},
  author={Steltner, Holger and Staats, Richard and Timmer, Jens and Vogel, Michael and Guttmann, Josef and Matthys, Heinrich and Christian Virchow, J},
  journal={American journal of respiratory and critical care medicine},
  volume={165},
  number={7},
  pages={940--944},
  year={2002},
  publisher={American Thoracic Society New York, NY}
}

@article{conceptdrift,
  title={The problem of concept drift: definitions and related work},
  author={Tsymbal, Alexey},
  journal={Computer Science Department, Trinity College Dublin},
  volume={106},
  number={2},
  pages={58},
  year={2004},
  publisher={Citeseer}
}

@Article{ncvreg,
    author = {Patrick Breheny and Jian Huang},
    title = {Coordinate descent algorithms for nonconvex penalized regression,
	with applications to biological feature selection},
    journal = {Annals of Applied Statistics},
    year = {2011},
    volume = {5},
    pages = {232--253},
    number = {1},
    doi = {10.1214/10-AOAS388},
    url = {https://doi.org/10.1214/10-AOAS388},
  }
  
@Article{randomforest,
    title = {Classification and Regression by randomForest},
    author = {Andy Liaw and Matthew Wiener},
    journal = {R News},
    year = {2002},
    volume = {2},
    number = {3},
    pages = {18-22},
    url = {https://CRAN.R-project.org/doc/Rnews/},
  }
  
@Article{kernlab,
    title = {kernlab -- An {S4} Package for Kernel Methods in {R}},
    author = {Alexandros Karatzoglou and Alex Smola and Kurt Hornik and Achim Zeileis},
    journal = {Journal of Statistical Software},
    year = {2004},
    volume = {11},
    number = {9},
    pages = {1--20},
    doi = {10.18637/jss.v011.i09},
  }

@Book{nnet,
    title = {Modern Applied Statistics with S},
    author = {W. N. Venables and B. D. Ripley},
    publisher = {Springer},
    edition = {Fourth},
    address = {New York},
    year = {2002},
    note = {ISBN 0-387-95457-0},
    url = {https://www.stats.ox.ac.uk/pub/MASS4/},
  }

@Manual{xgboost,
    title = {xgboost: Extreme Gradient Boosting},
    author = {Tianqi Chen and Tong He and Michael Benesty and Vadim Khotilovich and Yuan Tang and Hyunsu Cho and Kailong Chen and Rory Mitchell and Ignacio Cano and Tianyi Zhou and Mu Li and Junyuan Xie and Min Lin and Yifeng Geng and Yutian Li and Jiaming Yuan},
    year = {2024},
    note = {R package version 1.7.7.1},
    url = {https://CRAN.R-project.org/package=xgboost},
  }